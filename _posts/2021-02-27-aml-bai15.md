---
layout: post
title: "[LÃ½ Thuyáº¿t] BÃ i 15 Máº¡ng tháº§n kinh nhÃ¢n táº¡o - Artificial Neural Network"
subtitle: "Loáº¡t bÃ i há»c thuá»™c series AML"
date: 2021-03-07
author: "KyoHB"
header-img: "img/post-bg-aml.jpg"
tags: [AML]
---

Tráº£i qua má»™t loáº¡t cÃ¡c bÃ i há»c vá» cÃ¡c model ML truyá»n thá»‘ng, cháº¯c háº³n cÃ¡c báº¡n cÅ©ng Ä‘Ã£ ráº¥t mong Ä‘á»£i Ä‘áº¿n bÃ i há»c vá» máº¡ng tháº§n kinh nhÃ¢n táº¡o - Artificial Neural Network (ANN). ANN chÃ­nh lÃ  ná»n táº£ng cho há»c sÃ¢u (Deep learning) thá»© mÃ  Ä‘ang Ä‘Æ°á»£c bÃ¡o Ä‘Ã i nháº¯c Ä‘áº¿n tá»«ng ngÃ y, trong bÃ i há»c ngÃ y hÃ´m nay chÃºng ta sáº½ tÃ¬m hiá»ƒu ká»¹ hÆ¡n vá» ANN nhÃ© !

 
## Ã tÆ°á»Ÿng cá»§a ANN
ANN Ä‘Æ°á»£c hÃ¬nh thÃ nh báº±ng cÃ¡ch há»c theo cÃ¡ch mÃ  nÃ£o ngÆ°á»i xá»­ lÃ½ thÃ´ng tin. CÃ¡c nÆ¡ron sinh há»c cá»§a chÃºng ta tiáº¿p nháº­n thÃ´ng tin tá»« cÃ¡c sá»£i nhÃ¡nh (Dendrite) vÃ  Ä‘Æ°a vÃ o nhÃ¢n (Nucleus). NhÃ¢n nÆ¡ron sáº½ cÃ³ trÃ¡ch nhiá»‡m xá»­ lÃ½ cÃ¡c thÃ´ng tin Ä‘áº¿n vÃ  quyáº¿t Ä‘á»‹nh xem sáº½ gá»­i tÃ­n hiá»‡u gÃ¬ tiáº¿p theo Ä‘áº¿n cÃ¡c nÆ¡ron khÃ¡c thÃ´ng qua sá»£i trá»¥c (Axon).

![]({{ site.baseurl }}/img/in-post/aml/bnn.jpg)*Source: link.springer.com*
{:.image-caption}

Váº­y nÆ¡ron nhÃ¢n táº¡o thÃ¬ sáº½ cÃ³ cáº¥u táº¡o nhÆ° nÃ o ?

![]({{ site.baseurl }}/img/in-post/aml/ann.png)

NÆ¡ron nhÃ¢n táº¡o Ä‘Æ°á»£c thiáº¿t káº¿ tÆ°Æ¡ng Ä‘á»“ng nhÆ° nÆ¡ron sinh há»c. NÃ³ sáº½ tiáº¿p nháº­n cÃ¡c tÃ­n hiá»‡u Ä‘áº§u vÃ o, á»©ng vá»›i má»—i tÃ­n hiá»‡u nÃ y sáº½ lÃ  má»™t trá»ng sá»‘ (weight). CÃ¡c trá»ng sá»‘ (weight) nÃ y Ä‘áº¡i diá»‡n cho má»©c Ä‘á»™ quan trá»ng cá»§a tÃ­n hiá»‡u Ä‘áº§u vÃ o. CÃ¡c nÆ¡ron nhÃ¢n táº¡o sáº½ tá»•ng há»£p thÃ´ng tin tá»« cÃ¡c Ä‘áº§u vÃ o tÆ°Æ¡ng á»©ng theo trá»ng sá»‘ (weight) cá»§a chÃºng vÃ  Ä‘Æ°a vÃ o hÃ m kÃ­ch hoáº¡t (activation function). 
Máº¡ng tháº§n kinh nhÃ¢n táº¡o ANN Ä‘Æ°á»£c cáº¥u táº¡o tá»« nhiá»u cÃ¡c nÆ¡ron liÃªn káº¿t láº¡i vá»›i nhau. Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n (training), cÃ¡c trá»ng sá»‘ (weight) Ä‘áº§u vÃ o cá»§a cÃ¡c nÆ¡ron sáº½ Ä‘Æ°á»£c tinh chá»‰nh sao cho phÃ¹ há»£p vá»›i táº­p huáº¥n luyá»‡n (training set).

## NÆ¡ron nhÃ¢n táº¡o

Pháº§n tá»•ng há»£p trÃªn nÆ¡ron nhÃ¢n táº¡o chÃ­nh lÃ  tá»•ng trá»ng sá»‘ cá»§a cÃ¡c Ä‘áº§u vÃ o:

$u = \sum_{i=1}^{m} w_{i}x_{i}$
{:.formula}

Pháº§n tá»•ng há»£p $u$ sáº½ Ä‘Æ°á»£c cá»™ng thÃªm vá»›i há»‡ sá»‘ bias $b$. Bias Ä‘Ã³ng vai trÃ² nhÆ° há»‡ sá»‘ cháº·n (intercept) trong hÃ m sá»‘ tuyáº¿n tÃ­nh (linear function), nÃ³ giÃºp cho cÃ¡c Ä‘Æ°á»ng tuyáº¿n tÃ­nh (linear) tá»± do hÆ¡n, do Ä‘Ã³ nÃ³ cÃ²n Ä‘Æ°á»£c biáº¿t Ä‘áº¿n vá»›i má»™t cÃ¡i tÃªn lÃ  há»‡ sá»‘ tá»± do. HÃ¬nh dÆ°á»›i Ä‘Ã¢y sáº½ minh há»a rÃµ hÆ¡n vá» sá»± há»¯u Ã­ch cá»§a há»‡ sá»‘ nÃ y. Náº¿u khÃ´ng cÃ³ há»‡ sá»‘ bias $b$, Ä‘Æ°á»ng tuyáº¿n tÃ­nh (linear) cá»§a hÃ m sá»‘ $y = wx$ chá»‰ cÃ³ thá»ƒ Ä‘i qua gá»‘c tá»a Ä‘á»™ (origin), nhÆ° hÃ¬nh váº½ á»Ÿ bÃªn trÃ¡i. Báº±ng viá»‡c thÃªm há»‡ sá»‘ bias $b$, hÃ m sá»‘ $y = wx + b$ trá»Ÿ nÃªn tá»± do hÆ¡n nhÆ° hÃ¬nh bÃªn pháº£i, giÃºp cho model cá»§a chÃºng ta cÃ³ thá»ƒ dá»… dÃ ng thÃ­ch á»©ng (fit) vá»›i táº­p huáº¥n luyá»‡n (training set).


![]({{ site.baseurl }}/img/in-post/aml/ann_bias.png)*Source: geeksforgeeks.org*
{:.image-caption}

TrÆ°á»›c khi Ä‘Æ°á»£c Ä‘Æ°a Ä‘áº¿n cÃ¡c nÆ¡ron tiáº¿p theo hoáº·c Ä‘áº§u ra, cÃ¡c tÃ­n hiá»‡u Ä‘Æ°á»£c tá»•ng há»£p sáº½ Ä‘Æ°á»£c Ä‘Æ°a qua hÃ m kÃ­ch hoáº¡t (activation). ÄÆ°á»£c kÃ½ hiá»‡u toÃ¡n há»c nhu sau:

$y = \varphi (u + b)$
{:.formula}

ChÃºng ta sáº½ tÃ¬m hiá»u ká»¹ hÆ¡n vá» hÃ m kÃ­ch hoáº¡t á»Ÿ má»¥c tiáº¿p theo nhÃ© !

## Kiáº¿n trÃºc

Kiáº¿n trÃºc cá»§a má»™t ANN bao gá»“m 3 lá»›p (layer) Ä‘Ã³ lÃ  lá»›p Ä‘áº§u vÃ o (input layer), lá»›p áº©n (hidden layer) vÃ  lá»›p Ä‘áº§u ra (output layer). ThÃ´ng tin sáº½ Ä‘Æ°á»£c Ä‘i theo dáº¡ng truyá»n tháº³ng (feed forward) tá»« Ä‘áº§u vÃ o cho Ä‘áº¿n Ä‘áº§u ra. HÃ¬nh dÆ°á»›i Ä‘Ã¢y mÃ´ táº£ kiáº¿n trÃºc cÆ¡ báº£n cá»§a má»™t ANN:

![]({{ site.baseurl }}/img/in-post/aml/ann-architecture.png)*Source: researchgate.net*
{:.image-caption}

Lá»›p Ä‘áº§u vÃ o (input layer) sáº½ cÃ³ sá»‘ nÆ¡ron tÆ°Æ¡ng á»©ng vá»›i sá»‘ biáº¿n khÃ´ng phá»¥ thuá»™c (independent variable). Lá»›p Ä‘áº§u ra thÃ¬ sáº½ cÃ³ sá»‘ nÆ¡ron tÆ°Æ¡ng á»©ng vá»›i sá»‘ nhÃ³m (class) trong bÃ i toÃ¡n phÃ¢n loáº¡i (classification) hoáº·c chá»‰ lÃ  duy nháº¥t má»™t nÆ¡ron trong trÆ°á»ng há»£p bÃ i toÃ¡n há»“i quy (regression). CÃ²n lá»›p áº©n (hidden layer) thÃ¬ sao ?

CÃ¢u tráº£ lá»i lÃ  chÃºng ta cÃ³ thá»ƒ lá»±a chá»n tÃ¹y Ã½ cáº£ vá» sá»‘ lÆ°á»£ng lá»›p (layer) vÃ  sá»‘ lÆ°á»£ng nÆ¡ron trong má»—i lá»›p (layer) mÃ  khÃ´ng cÃ³ má»™t quy Ä‘á»‹nh nÃ o cá»¥ thá»ƒ. ThÆ°á»ng thÃ¬ sá»‘ lÆ°á»£ng nÆ¡ron hay Ä‘Æ°á»£c chá»n sáº½ lÃ  lÅ©y thá»«a cá»§a 2, nhÆ° 16, 32, 64, 128,... mÃ  khÃ´ng cÃ³ má»™t lÃ½ do nÃ o cá»¥ thá»ƒ. Vá» sá»‘ lÆ°á»£ng lá»›p áº©n (hidden layer) thÃ¬ cáº§n pháº£i cÃ¢n nháº¯c, cÃ ng nhiá»u lá»›p áº©n (hidden layer) sáº½ cÃ ng khiáº¿n cho model quÃ¡ vá»«a váº·n (overfit) theo táº­p huáº¥n luyá»‡n (training set). HÃ¬nh sau sáº½ mÃ´ táº£ sá»± thay Ä‘á»•i cá»§a vÃ¹ng quyáº¿t Ä‘á»‹nh khi tÄƒng sá»‘ lÆ°á»£ng lá»›p áº©n (hidden layer).

![]({{ site.baseurl }}/img/in-post/aml/ann-hidden.jpg)*Source:slazebni.cs.illinois.edu*
{:.image-caption}

## Huáº¥n luyá»‡n

QuÃ¡ trÃ¬nh huáº¥n luyá»‡n (training) ANN Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua cÆ¡ cháº¿ truyá»n ngÆ°á»£c (backpropagation). Sai sá»‘ giá»¯a lá»›p Ä‘áº§u ra (output layer) vÃ  nhÃ£n thá»±c táº¿ (groundtruth) sáº½ Ä‘Æ°á»£c truyá»n ngÆ°á»£c trá»Ÿ láº¡i nháº±m tinh chá»‰nh cÃ¡c trá»ng sá»‘ (weight) giá»¯a cÃ¡c káº¿t ná»‘i. CÃ¢u há»i Ä‘Æ°á»£c Ä‘áº·t ra á»Ÿ Ä‘Ã¢y lÃ  Ä‘iá»u chá»‰nh tÄƒng giáº£m cÃ¡c trá»ng sá»‘ (weight) nhÆ° nÃ o Ä‘á»ƒ giáº£m thiá»ƒu sai sá»‘ ?

![]({{ site.baseurl }}/img/in-post/aml/ann-fb.png) *Source:smarterbeta.wordpress.com*
{:.image-caption}

Viá»‡c nÃ y sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n nhá» phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c gá»i lÃ  suy giáº£m Ä‘á»™ dá»‘c (gradient descent). CÃ²n nhá»› nhá»¯ng nÄƒm cáº¥p 3 chÃºng ta há»c phÆ°Æ¡ng phÃ¡p Ä‘áº¡o hÃ m vÃ  Ã¡p dá»¥ng nÃ³ vÃ o cÃ¡c phÆ°Æ¡ng trÃ¬nh nháº±m tÃ¬m ra Ä‘iá»ƒm cá»±c tiá»ƒu (minumum point) vÃ  Ä‘iá»ƒm cá»±c Ä‘áº¡i (maximum point), bÃ¢y giá» chÃºng ta cÅ©ng sáº½ sá»­ dá»¥ng nÃ³ nháº±m tÃ¬m hÆ°á»›ng Ä‘i cho cÃ¡c trá»ng sá»‘ (weight). HÃ£y tÆ°á»Ÿng tÆ°á»£ng lÃºc nÃ y chÃºng ta Ä‘ang Ä‘i xuá»‘ng má»™t con dá»‘c Ä‘á»ƒ xuá»‘ng Ä‘Æ°á»£c Ä‘áº¿n chÃ¢n dá»‘c pháº§n tháº¥p nháº¥t cÅ©ng tÆ°Æ¡ng á»©ng vá»›i Ä‘iá»ƒm mÃ  sai sá»‘ tháº¥p nháº¥p, suy giáº£m Ä‘á»™ dá»‘c (gradient descent) sá»­ dá»¥ng Ä‘áº¡o hÃ m (derivative) nhÆ° má»™t cÃ¢y gáº­y chá»‰ Ä‘Æ°á»ng, khi Ä‘áº¡o hÃ m (derivative) Ã¢m, chÃºng ta gia tÄƒng trá»ng sá»‘ (weight), khi Ä‘áº¡o hÃ m (derivative) dÆ°Æ¡ng, chÃºng ta sáº½ giáº£m trá»ng sá»‘ (weight) láº¡i, nhÆ° mÃ´ táº£ hÃ¬nh dÆ°á»›i Ä‘Ã¢y.

![]({{ site.baseurl }}/img/in-post/aml/ann-gd.jpeg)*Source: datahacker.rs*
{:.image-caption}

Trong hÃ¬nh trÃªn chÃºng ta tháº¥y cÃ¡c Ä‘iá»ƒm mÃ u xanh lÃ  cÃ¡c Ä‘iá»ƒm dá»‹ch chuyá»ƒn cá»§a sai sá»‘, khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘iá»ƒm nÃ y Ä‘Æ°á»£c Ä‘áº·c trÆ°ng bá»Ÿi tham sá»‘ tá»‘c Ä‘á»™ há»c (learning rate). Tá»‘c Ä‘á»™ há»c (learning rate) cÃ ng lá»›n, cÃ¡c bÆ°á»›c dá»‹ch chuyá»ƒn cÃ ng dÃ i hÆ¡n, dá»… khiáº¿n cho sai sá»‘ báº­t nháº£y há»—n loáº¡n, cÃ³ thá»ƒ bá» qua máº¥t Ä‘iá»ƒm cá»±c tiá»ƒu (minimum point). NgÆ°á»£c láº¡i khi tá»‘c Ä‘á»™ há»c (learning rate) nhá», cÃ¡c bÆ°á»›c dá»‹ch chuyá»ƒn ngáº¯n láº¡i, lÃ¢u Ä‘áº¡t Ä‘áº¿n Ä‘Æ°á»£c Ä‘iá»ƒm cá»±c tiá»ƒu, cÅ©ng nhÆ° dá»… bá»‹ káº¹t láº¡i á»Ÿ cÃ¡c Ä‘iá»ƒm cá»±c tiá»ƒu cá»¥c bá»™ (local minimum), thay vÃ¬ Ä‘áº¡t Ä‘áº¿n Ä‘iá»ƒm cá»±c tiá»ƒu toÃ n cá»¥c (global minimum). VÃ¬ váº­y viá»‡c lá»±a chá»n má»™t tá»‘c Ä‘á»™ há»c (learning rate) phÃ¹ há»£p lÃ  ráº¥t cáº§n thiáº¿t, hoáº·c cáº§n pháº£i sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p linh hoáº¡t (adaptive) nháº±m thay Ä‘á»•i tá»‘c Ä‘á»™ há»c (learning rate) theo tá»«ng giai Ä‘oáº¡n.

![]({{ site.baseurl }}/img/in-post/aml/ann-lr.jpg)*Source: saugatbhattarai.com.np*
{:.image-caption}

## HÃ m kÃ­ch hoáº¡t

HÃ m kÃ­ch hoáº¡t (activation function) luÃ´n xuáº¥t hiá»‡n á»Ÿ Ä‘áº§u ra cá»§a má»—i nÆ¡ron, nÃ³ sáº½ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh xem cÃ³ Ä‘Æ°a thÃ´ng tin Ä‘Ã£ Ä‘Æ°á»£c tá»•ng há»£p sang nÆ¡ron khÃ¡c hay khÃ´ng. NhÃ¬n má»™t cÃ¡ch tá»•ng quÃ¡t nÃ³ giá»‘ng nhÆ° má»™t chiáº¿c cÃ´ng táº¯c giÃºp báº­t táº¯t nÆ¡ron. CÃ³ ráº¥t nhiá»u loáº¡i hÃ m kÃ­ch hoáº¡t (activation function) vÃ  nhá»¯ng nhÃ  nghiÃªn cá»©u váº«n Ä‘ang tÃ¬m nhá»¯ng hÃ m kÃ­ch hoáº¡t (activation function) má»›i hiá»‡u quáº£ hÆ¡n, 4 hÃ m kÃ­ch hoáº¡t (activation function) dÆ°á»›i Ä‘Ã¢y lÃ  thÃ´ng dá»¥ng hÆ¡n cáº£:

![]({{ site.baseurl }}/img/in-post/aml/ann_atvfunc.jpg)*Source: docs.paperspace.com*
{:.image-caption}

Trong Ä‘Ã³ Sigmoid-Softmax vÃ  Linear Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ cÃ¡c nÆ¡ron Ä‘áº§u ra, tÆ°Æ¡ng á»©ng cho bÃ i toÃ n phÃ¢n loáº¡i (classification) hay há»“i quy (regression). CÃ¡c hÃ m kÃ­ch hoáº¡t (activation function) Sigmoid, Tanh vÃ  ReLU cÃ²n Ä‘Æ°á»£c gá»i lÃ  cÃ¡c hÃ m kÃ­ch hoáº¡t phi tuyáº¿n tÃ­nh (non-linear activation function) vÃ  Ä‘Æ°á»£c dÃ¹ng trong káº¿t ná»‘i giá»¯a cÃ¡c nÆ¡ron vá»›i nhau. Náº¿u nhÆ° hÃ m kÃ­ch hoáº¡t (activation function) Linear Ä‘Æ°á»£c sá»­ dá»¥ng trong káº¿t ná»‘i giá»¯a cÃ¡c neuron, chÃºng ta sáº½ vÃ´ hÃ¬nh chÃºng quy táº¥t cáº£ cÃ¡c lá»›p nÆ¡ron vá» má»™t vÃ¬ tá»•ng há»£p cÃ¡c hÃ m tuyáº¿n tÃ­nh thÃ¬ váº«n sáº½ lÃ  má»™t hÃ m tuyáº¿n tÃ­nh. ÄÃ³ lÃ  lÃ½ do mÃ  cÃ¡c hÃ m kÃ­ch hoáº¡t phi tuyáº¿n tÃ­nh (non-linear activation function) Ä‘Æ°á»£c sá»­ dá»¥ng, tá»« Ä‘Ã¢y chÃºng ta cÃ³ thá»ƒ káº¿t há»£p cÃ¡c lá»›p nÆ¡ron má»™t cÃ¡ch Ä‘Ãºng nghÄ©a, mang Ä‘áº¿n kháº£ nÄƒng xá»­ lÃ½ nhá»¯ng dá»¯ liá»‡u phá»©c táº¡p vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao.

Trong sá»‘ 3 hÃ m kÃ­ch hoáº¡t phi tuyáº¿n tÃ­nh (non-linear activation function) trÃªn, ReLU lÃ  hÃ m kÃ­ch hoáº¡t thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng hÆ¡n cáº£, do hiá»‡n tÆ°á»£ng tiÃªu biáº¿n Ä‘á»™ dá»‘c (vanishing gradient) á»Ÿ hÃ m kÃ­ch hoáº¡t (activation function) Sigmoid vÃ  Tanh. Hiá»‡n tÆ°á»£ng tiÃªu biáº¿n Ä‘á»™ dá»‘c (vanishing gradient) xáº£y ra Ä‘á»‘i vá»›i cÃ¡c giÃ¡ trá»‹ á»Ÿ nhá»¯ng Ä‘iá»ƒm bÃ£o hÃ²a cá»§a hÃ m sá»‘ Tanh vÃ  Sigmoid, lÃºc nÃ y Ä‘áº¡o hÃ m (derivative) trá»Ÿ nÃªn ráº¥t nhá», háº­u quáº£ lÃ m cho viá»‡c thay Ä‘á»•i cÃ¡c trá»ng sá»‘ (weight) trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n (training) gáº§n nhÆ° báº±ng 0, viá»‡c huáº¥n luyá»‡n (training) cá»§a model gáº§n nhÆ° khÃ´ng cÃ³ tÃ¡c dá»¥ng. GiÃ¡o sÆ° dáº¡y mÃ¬nh mÃ´n há»c nÃ y cÃ³ má»™t vÃ­ dá»¥ ráº¥t hay vá» hiá»‡n tÆ°á»£ng nÃ y, Ä‘Ã³ lÃ  nhÆ° khi báº¡n nháº¥n ga má»™t chiáº¿c xe nhÆ°ng nÃ³ gáº§n nhÆ° khÃ´ng nhÃºc nhÃ­ch vá» phÃ­a trÆ°á»›c. Vá»›i hÃ m kÃ­ch hoáº¡t (activation function) ReLU, cÃ¡c giÃ¡ trá»‹ dÆ°Æ¡ng giá» Ä‘Ã¢y Ä‘Æ°á»£c giá»¯ nguyÃªn giÃ¡ trá»‹, khÃ´ng cÃ²n nhá»¯ng Ä‘iá»ƒm bÃ£o hÃ²a ná»¯a, Ä‘áº¡o hÃ m (derivative) luÃ´n cÃ³ Ä‘á»™ lá»›n Ä‘á»§ tá»‘t cho viá»‡c huáº¥n luyá»‡n (training).

![]({{ site.baseurl }}/img/in-post/aml/ann-vg.png)*Source: towardsdatascience.com*
{:.image-caption}

# Káº¿t

NhÆ° váº­y trong bÃ i há»c ngÃ y hÃ´m nay chÃºng ta Ä‘Ã£ tÃ¬m hiá»ƒu vá» Ã½ tÆ°á»Ÿng, cáº¥u táº¡o, cÃ¡ch váº­n hÃ nh vÃ  huáº¥n luyá»‡n má»™t máº¡ng tháº§n kinh nhÃ¢n táº¡o ANN, trong bÃ i há»c tiáº¿p theo chÃºng ta sáº½ báº¯t tay xÃ¢y dá»±ng má»™t ANN Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n phÃ¢n loáº¡i nhÃ© ! Náº¿u cÃ²n gÃ¬ tháº¯c máº¯c, Ä‘á»«ng ngáº¡i ngáº§n hÃ£y nháº¯n tin vá»›i admin qua fanpage Ä‘á»ƒ Ä‘Æ°á»£c giáº£i Ä‘Ã¡p ngay nhÃ© ğŸ’ª